"""
Factory para inicializa√ß√£o de LLM com fallback inteligente.

Este m√≥dulo implementa a l√≥gica de fallback que prioriza Azure OpenAI
se dispon√≠vel, e utiliza OpenAI padr√£o como fallback. Garante que a
aplica√ß√£o funcione tanto em ambiente corporativo quanto pessoal.
"""

import os
import logging
from typing import Optional
from dotenv import load_dotenv

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

try:
    from langchain_openai import ChatOpenAI, AzureChatOpenAI
    from langchain_core.language_models.chat_models import BaseChatModel
except ImportError:
    raise ImportError(
        "LangChain OpenAI n√£o est√° instalado. Execute: pip install langchain-openai"
    )


class LLMInitializationError(Exception):
    """Exce√ß√£o levantada quando n√£o √© poss√≠vel inicializar nenhum provedor LLM."""
    pass


class LLMFactory:
    """
    Factory respons√°vel pela cria√ß√£o de inst√¢ncias LLM com fallback autom√°tico.
    
    Implementa a l√≥gica de verifica√ß√£o de credenciais e inicializa√ß√£o
    do provedor adequado com prioridade para Azure OpenAI em ambiente corporativo.

    L√≥gica de fallback:
    1. Tenta Azure OpenAI se credenciais estiverem dispon√≠veis
    2. Tenta OpenAI padr√£o como fallback
    3. Falha apenas se nenhum dos dois estiver configurado
    """
    
    def __init__(self):
        """Inicializa o factory com logger configurado."""
        self.logger = logging.getLogger(__name__)
    
    def _check_azure_credentials(self) -> bool:
        """
        Verifica se todas as credenciais necess√°rias do Azure OpenAI est√£o presentes.
        
        Suporta tanto AZURE_OPENAI_ENDPOINT quanto AZURE_OPENAI_API_BASE para
        compatibilidade com diferentes configura√ß√µes.
        
        Returns:
            bool: True se todas as credenciais est√£o dispon√≠veis, False caso contr√°rio
        """
        # Vari√°veis obrigat√≥rias
        required_vars = [
            "AZURE_OPENAI_API_KEY",
            "AZURE_OPENAI_DEPLOYMENT_NAME",
            "AZURE_OPENAI_API_VERSION"
        ]
        
        missing_vars = []
        for var in required_vars:
            if not os.getenv(var):
                missing_vars.append(var)
        
        # Verificar se pelo menos um dos endpoints est√° configurado
        endpoint = os.getenv("AZURE_OPENAI_ENDPOINT") or os.getenv("AZURE_OPENAI_API_BASE")
        if not endpoint:
            missing_vars.append("AZURE_OPENAI_ENDPOINT ou AZURE_OPENAI_API_BASE")
        
        if missing_vars:
            self.logger.debug(
                f"Credenciais do Azure OpenAI ausentes: {', '.join(missing_vars)}. "
                "Tentando fallback para OpenAI padr√£o."
            )
            return False
        
        return True
    
    def _check_openai_credentials(self) -> bool:
        """
        Verifica se as credenciais necess√°rias da OpenAI padr√£o est√£o presentes.
        
        Returns:
            bool: True se as credenciais est√£o dispon√≠veis, False caso contr√°rio
        """
        required_vars = ["OPENAI_API_KEY", "OPENAI_MODEL_NAME"]
        
        missing_vars = []
        for var in required_vars:
            if not os.getenv(var):
                missing_vars.append(var)
        
        if missing_vars:
            self.logger.debug(
                f"Credenciais da OpenAI padr√£o ausentes: {', '.join(missing_vars)}."
            )
            return False
        
        return True
    
    def _create_azure_llm(self) -> AzureChatOpenAI:
        """
        Cria uma inst√¢ncia do Azure OpenAI LLM.
        
        Returns:
            AzureChatOpenAI: Inst√¢ncia configurada do Azure OpenAI
        """
        try:
            # Suportar tanto AZURE_OPENAI_ENDPOINT quanto AZURE_OPENAI_API_BASE
            endpoint = os.getenv("AZURE_OPENAI_ENDPOINT") or os.getenv("AZURE_OPENAI_API_BASE")
            
            azure_llm = AzureChatOpenAI(
                api_key=os.getenv("AZURE_OPENAI_API_KEY"),
                azure_endpoint=endpoint,
                deployment_name=os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME"),
                api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
                temperature=0.7,
                max_tokens=15000,
                timeout=120,  # 2 minutos de timeout para chamadas LLM
                max_retries=3
            )
            
            self.logger.info(
                f"‚úÖ Azure OpenAI inicializado com sucesso - "
                f"Endpoint: {endpoint} | "
                f"Deployment: {os.getenv('AZURE_OPENAI_DEPLOYMENT_NAME')}"
            )
            
            return azure_llm
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao inicializar Azure OpenAI: {e}")
            raise LLMInitializationError(f"Falha na inicializa√ß√£o do Azure OpenAI: {e}")
    
    def _create_openai_llm(self) -> ChatOpenAI:
        """
        Cria uma inst√¢ncia do OpenAI padr√£o LLM.
        
        Returns:
            ChatOpenAI: Inst√¢ncia configurada da OpenAI padr√£o
        """
        try:
            openai_llm = ChatOpenAI(
                api_key=os.getenv("OPENAI_API_KEY"),
                model=os.getenv("OPENAI_MODEL_NAME", "gpt-4"),
                temperature=0.7,
                max_tokens=15000,
                timeout=120,  # 2 minutos de timeout para chamadas LLM
                max_retries=3
            )
            
            self.logger.info(
                f"‚úÖ OpenAI padr√£o inicializado com sucesso - "
                f"Modelo: {os.getenv('OPENAI_MODEL_NAME', 'gpt-4')}"
            )
            
            return openai_llm
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao inicializar OpenAI padr√£o: {e}")
            raise LLMInitializationError(f"Falha na inicializa√ß√£o da OpenAI padr√£o: {e}")
    
    def initialize_llm_provider(self) -> BaseChatModel:
        """
        Inicializa um provedor LLM adequado com l√≥gica de fallback inteligente.
        
        A fun√ß√£o tenta primeiro utilizar Azure OpenAI se estiver configurado.
        Se o Azure n√£o estiver dispon√≠vel, utiliza OpenAI padr√£o como fallback.
        Apenas emite erro se nenhum dos dois estiver configurado corretamente.
        
        Returns:
            BaseChatModel: Inst√¢ncia configurada do LLM (Azure ou OpenAI padr√£o)
            
        Raises:
            LLMInitializationError: Quando nenhum provedor pode ser inicializado
        """
        self.logger.info("üîÑ Iniciando processo de inicializa√ß√£o do LLM...")
        
        # Verificar e tentar Azure OpenAI primeiro
        azure_available = self._check_azure_credentials()
        if azure_available:
            try:
                self.logger.info("‚úÖ Credenciais Azure OpenAI detectadas, inicializando...")
                return self._create_azure_llm()
            except LLMInitializationError as e:
                self.logger.warning(f"‚ö†Ô∏è Falha no Azure OpenAI: {e}, tentando fallback...")
        
        # Tentar OpenAI padr√£o como fallback
        openai_available = self._check_openai_credentials()
        if openai_available:
            try:
                self.logger.info("‚úÖ Credenciais OpenAI padr√£o detectadas, inicializando...")
                return self._create_openai_llm()
            except LLMInitializationError as e:
                self.logger.error(f"‚ùå Falha tamb√©m na OpenAI padr√£o: {e}")
        
        # Nenhum provedor dispon√≠vel - gerar mensagem de erro apropriada
        error_parts = ["‚ùå Nenhum provedor LLM pode ser inicializado."]
        
        if not azure_available and not openai_available:
            error_parts.append("Nenhuma credencial de LLM configurada no ambiente.")
        elif not azure_available:
            error_parts.append("Credenciais Azure OpenAI ausentes ou incompletas.")
        elif not openai_available:
            error_parts.append("Credenciais OpenAI padr√£o ausentes ou incompletas.")
        
        error_parts.append(
            "Configure pelo menos um dos seguintes conjuntos de vari√°veis no arquivo .env:\n"
            "- Azure OpenAI: AZURE_OPENAI_API_KEY, AZURE_OPENAI_API_BASE, "
            "AZURE_OPENAI_DEPLOYMENT_NAME, AZURE_OPENAI_API_VERSION\n"
            "- OpenAI padr√£o: OPENAI_API_KEY, OPENAI_MODEL_NAME"
        )
        
        error_msg = " ".join(error_parts)
        self.logger.error(error_msg)
        raise LLMInitializationError(error_msg)
        
    def load_llm(self) -> BaseChatModel:
        """
        Carrega uma inst√¢ncia de LLM com fallback autom√°tico.
        
        Mantido por compatibilidade com c√≥digo existente.
        Redireciona para o novo m√©todo initialize_llm_provider().
        
        Returns:
            BaseChatModel: Inst√¢ncia configurada do LLM (Azure ou OpenAI padr√£o)
            
        Raises:
            LLMInitializationError: Quando nenhum provedor pode ser inicializado
        """
        return self.initialize_llm_provider()


# Novos prompts integrados para StoryCreator
# O LLM agora deve gerar hist√≥rias mais detalhadas, incluindo explica√ß√µes para prioridade e estimativa.


# Inst√¢ncia global do factory
llm_factory = LLMFactory()


def load_llm() -> BaseChatModel:
    """
    Fun√ß√£o de conveni√™ncia para carregar o LLM.
    
    Returns:
        BaseChatModel: Inst√¢ncia configurada do LLM
        
    Raises:
        LLMInitializationError: Quando nenhum provedor pode ser inicializado
    """
    return llm_factory.load_llm()
